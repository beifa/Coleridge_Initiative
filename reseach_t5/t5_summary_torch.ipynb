{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1623781976429,
     "user": {
      "displayName": "beifa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtHChsX9hHYvtnlB_68A7jsW4UBJFWRmr3FDc9=s64",
      "userId": "10643883607469925396"
     },
     "user_tz": -180
    },
    "id": "pIMOy0yCV-gL",
    "outputId": "74e1c30d-cb06-424d-cec2-817d2d48b6fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_3UZqLw61B9"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1Mu6OEa71I_"
   },
   "outputs": [],
   "source": [
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1623781988717,
     "user": {
      "displayName": "beifa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtHChsX9hHYvtnlB_68A7jsW4UBJFWRmr3FDc9=s64",
      "userId": "10643883607469925396"
     },
     "user_tz": -180
    },
    "id": "nqC3b3SO8BeC",
    "outputId": "6802957f-06bb-4a43-ad4d-597e132a25a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeifa\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3427,
     "status": "ok",
     "timestamp": 1623781992142,
     "user": {
      "displayName": "beifa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtHChsX9hHYvtnlB_68A7jsW4UBJFWRmr3FDc9=s64",
      "userId": "10643883607469925396"
     },
     "user_tz": -180
    },
    "id": "bqh7j3n878Id",
    "outputId": "73e4ab89-225b-4af9-ed38-8db38495f6f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu101\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import os, glob, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (T5ForConditionalGeneration,\n",
    "                          AdamW,\n",
    "                          T5TokenizerFast as token)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "pl.seed_everything(13)\n",
    "print(torch.__version__)\n",
    "PATH = '/content/drive/MyDrive/Coleridge_Initiative/input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1623781992143,
     "user": {
      "displayName": "beifa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtHChsX9hHYvtnlB_68A7jsW4UBJFWRmr3FDc9=s64",
      "userId": "10643883607469925396"
     },
     "user_tz": -180
    },
    "id": "I33ZjK0CHwMi",
    "outputId": "10d50058-8618-4664-d4f4-6c49aae07145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 15 18:33:11 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   51C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHf1tD8WVPrf"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "executionInfo": {
     "elapsed": 1633,
     "status": "ok",
     "timestamp": 1623781993766,
     "user": {
      "displayName": "beifa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtHChsX9hHYvtnlB_68A7jsW4UBJFWRmr3FDc9=s64",
      "userId": "10643883607469925396"
     },
     "user_tz": -180
    },
    "id": "zrDTSd8iV7qz",
    "outputId": "6658ae74-66f6-4ba9-ae7c-697996c12281"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40bf999d-8537-47f7-8913-bd67b060146d</td>\n",
       "      <td>the quality of educational programs has been a...</td>\n",
       "      <td>trends in international mathematics and scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40bf999d-8537-47f7-8913-bd67b060146d</td>\n",
       "      <td>knowledge about the potential usefulness of th...</td>\n",
       "      <td>conclusions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631ebad-191f-44d7-9c21-89b4f84852dc</td>\n",
       "      <td>as mentioned above the sars cov 2 s glycoprote...</td>\n",
       "      <td>genome sequence of sars cov 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43f2034e-9220-4103-84bf-79f67b75f29c</td>\n",
       "      <td>attributed to a cold core anticyclonic eddy ra...</td>\n",
       "      <td>journal of geophysical research oceans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43f2034e-9220-4103-84bf-79f67b75f29c</td>\n",
       "      <td>deep arctic quality profiles between and from ...</td>\n",
       "      <td>world ocean database</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  ...                                            summary\n",
       "0  40bf999d-8537-47f7-8913-bd67b060146d  ...  trends in international mathematics and scienc...\n",
       "1  40bf999d-8537-47f7-8913-bd67b060146d  ...                                        conclusions\n",
       "2  5631ebad-191f-44d7-9c21-89b4f84852dc  ...                      genome sequence of sars cov 2\n",
       "3  43f2034e-9220-4103-84bf-79f67b75f29c  ...             journal of geophysical research oceans\n",
       "4  43f2034e-9220-4103-84bf-79f67b75f29c  ...                               world ocean database\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df[['id', 'text', 'answer']]\n",
    "# df.columns = ['id', 'text', 'summary']\n",
    "# df.to_csv('/content/drive/MyDrive/Coleridge_Initiative/input/v6_data_summary.csv', index= False)\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Coleridge_Initiative/input/v6_data_summary.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "executionInfo": {
     "elapsed": 2327,
     "status": "ok",
     "timestamp": 1623781996089,
     "user": {
      "displayName": "beifa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtHChsX9hHYvtnlB_68A7jsW4UBJFWRmr3FDc9=s64",
      "userId": "10643883607469925396"
     },
     "user_tz": -180
    },
    "id": "skTdDeA19iuh",
    "outputId": "230ab277-5a02-4fc1-9d61-548da4b6b356"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.32<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">T5_t5-small_summary_LR:0.0003_BATCH:3_FRAC_DATA:0.9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/beifa/ci\" target=\"_blank\">https://wandb.ai/beifa/ci</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/beifa/ci/runs/30a4zgk7\" target=\"_blank\">https://wandb.ai/beifa/ci/runs/30a4zgk7</a><br/>\n",
       "                Run data is saved locally in <code>/content/wandb/run-20210615_183313-30a4zgk7</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH = 3\n",
    "EPOCHS = 6\n",
    "FRAC = 0.9\n",
    "\n",
    "config={\n",
    "    'type_model': 'summary',\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"architecture\": \"T5\",\n",
    "    'model': 't5-small', #'t5-base', # t5-small\n",
    "    \"dataset\": \"Coleridge Initiative \",\n",
    "    'tex_max_len': 512,\n",
    "    'summar_max_len': 18,\n",
    "    'batch_size' : BATCH,\n",
    "    'epoch':EPOCHS,\n",
    "    'device': 'cuda'\n",
    "}\n",
    "\n",
    "\n",
    "wandb.init(project=\"ci\",\n",
    "           config=config,\n",
    "           name =  f\"{config['architecture']}_{config['model']}_{config['type_model']}\"+\\\n",
    "                   f\"_LR:{config['learning_rate']}_BATCH:{BATCH}_FRAC_DATA:{FRAC}\"\n",
    "           )\n",
    "config_w = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ceo8GpbU_Uuo"
   },
   "outputs": [],
   "source": [
    "class CI_Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: token,\n",
    "        config\n",
    "    ):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data        \n",
    "        self.tex_max_len = config['tex_max_len']\n",
    "        self.summar_max_len = config['summar_max_len']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index:int):\n",
    "        txt = self.data.iloc[index]\n",
    "\n",
    "        encode_txt = token(            \n",
    "            txt['text'],\n",
    "            max_length = self.tex_max_len, \n",
    "            padding= 'max_length',\n",
    "            truncation = True, #'only_second',\n",
    "            return_attention_mask = True,\n",
    "            add_special_tokens =True,\n",
    "            return_tensors = 'pt'\n",
    "            )\n",
    "        \n",
    "        encode_summary = token( \n",
    "            txt['summary'],\n",
    "            max_length = self.summar_max_len,\n",
    "            padding= 'max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            add_special_tokens =True,\n",
    "            return_tensors = 'pt'\n",
    "            )\n",
    "        \n",
    "        labels = encode_summary['input_ids']\n",
    "        labels[labels == 0] = -100\n",
    "\n",
    "        return dict(          \n",
    "            text = txt['text'],\n",
    "            summary = txt['summary'],\n",
    "            input_ids = encode_txt['input_ids'].flatten(),\n",
    "            attention_mask = encode_txt['attention_mask'].flatten(),\n",
    "            labels = labels.flatten(),\n",
    "            label_attention_mask = encode_summary['attention_mask'].flatten(),\n",
    "\n",
    "            )\n",
    "        \n",
    "class CI(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()   \n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(config['model'], return_dict = True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, summary_attention_mask, labels = None):\n",
    "        out = self.model(input_ids = input_ids,\n",
    "                         attention_mask = attention_mask,\n",
    "                         labels = labels,\n",
    "                         decoder_attention_mask = summary_attention_mask\n",
    "                         )\n",
    "        return out.loss, out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1623781996439,
     "user": {
      "displayName": "beifa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtHChsX9hHYvtnlB_68A7jsW4UBJFWRmr3FDc9=s64",
      "userId": "10643883607469925396"
     },
     "user_tz": -180
    },
    "id": "mBb9cgbwAC76",
    "outputId": "14c33272-c695-4aa6-90af-a8f847c3b187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quality of educational programs has been an object of debate and research around the world initiatives such as the program for international student assessment pisa and the trends in international mathematics and science study timss show that international organizations such as the organization for economic co operation and development oecd and the international association for the evaluation of educational achievement iea are trying to verify whether schools are adequately preparing their students by comparing their performances aiming to highlight the strengths and weaknesses among the educational systems of different countries higher education has also been the object of quality evaluations around the world ursin huusko aittola kiviniemi muhonen van kemenade pupius hardjono governmental and non governmental organizations have developed ways to certify institutional quality through evaluation or accreditation processes examples of these organizations include the european association for quality assurance in higher education enqa the quality assurance agency for higher education qaa the association to advance collegiate schools of business aacsb and the national institute of educational studies and research an sio teixeira inep many higher education institutions are applying for an iso certificate as a way to assure their quality lundquist ursin et al van kemenade et al but the most popular way to obtain evidence of quality in higher education programs is through external evaluation van kemenade et al external program evaluations are implemented with the goal of producing information that helps to better comprehend how activities processes and outcomes are contributing to the attainment of an organization s primary objectives therefore if properly used evaluations can potentially serve as an information system that can help educational institutions achieve their goals and correct possible deviations in their operations additionally according to the utilization focused evaluation literature educational programs could benefit from the evaluation report utilization because the ultimate purpose of evaluation is to improve programs and increase the quality of decisions made patton p the definition of evaluation use has been widely discussed in utilization focused evaluation theory among the many concepts of evaluation use that of cousins and leithwood perfectly fits the purpose of the present study this concept states that the mere psychological processing of evaluation results constitutes use without necessarily informing decisions dictating actions or changing thinking cousins leithwood p in an attempt to better distinguish the evaluation uses presented in the literature leviton and hughes summarized the categories for the most frequent uses described at that time and classified them into the current and broadly known types of use which include conceptual use instrumental use and persuasive use this nomenclature is generally accepted when describing the uses of evaluation findings alkin taut preskill caracelli the conceptual type of use also known as enlightenment braskamp owen lambert refers to improving the understanding of program aspects such as its participants its context or its outcomes through the evaluation the conceptual use is also related to developing new views of the program and identifying problems alkin braskamp henry mark the instrumental use perhaps the earliest type of use examined in the literature johnson p is related to the\n",
      "trends in international mathematics and science study\n",
      "tensor([   8,  463,   13, 3472, 1356,   65,  118,   46, 3735,   13])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "MODEL = config['model']\n",
    "token = token.from_pretrained(MODEL)\n",
    "exampe_dataset = CI_Dataset(df, token, config)\n",
    "\n",
    "for data in exampe_dataset:\n",
    "    print(data['text'])\n",
    "    print(data['summary'])\n",
    "    print(data['input_ids'][:10])\n",
    "    print(data['attention_mask'][:10]) \n",
    "    print(data['label_attention_mask'][:10])  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVHI6v50f5lg"
   },
   "outputs": [],
   "source": [
    "# text_count, summary_count = [], []\n",
    "# for _, row in df.iterrows():\n",
    "#     txt_count = len(token.encode(row['text']))\n",
    "#     text_count.append(txt_count)\n",
    "#     sm_count = len(token.encode(row['summary']))\n",
    "#     summary_count.append(sm_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHqZ8IpXiGgs"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2)\n",
    "# ax[0].hist(text_count, bins=100)\n",
    "# ax[0].set_title('txt count token')\n",
    "# ax[1].hist(summary_count, bins=20)\n",
    "# ax[1].set_title('summary count token');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBOxEAxLYnOn"
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "def make_current_date()->str:\n",
    "    x = datetime.datetime.now()\n",
    "    x = str(x.date())\n",
    "    return x.replace('-', '_')\n",
    "\n",
    "\n",
    "def loss_func(pred, target, config):\n",
    "    tmp = []\n",
    "    MODEL = config['model']\n",
    "    tokenizer = token.from_pretrained(MODEL)\n",
    "    for i in range(len(pred)):\n",
    "        out_decode = tokenizer.decode(np.argmax(pred[i], axis = 1),\n",
    "                                  skip_special_tokens=True, \n",
    "                                  clean_up_tokenization_spaces=True)       \n",
    "        tmp.append(jaccard(''.join(out_decode), target[i]))\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, config, scheduler = None):\n",
    "    model.train()\n",
    "    o = []\n",
    "    for _, txt in enumerate(data_loader):        \n",
    "        input_ids=txt['input_ids'].to(config['device'])\n",
    "        mask=txt['attention_mask'].to(config['device'])\n",
    "        labels = txt['labels'].to(config['device'])\n",
    "        labels_mask = txt['label_attention_mask'].to(config['device'])\n",
    "        optimizer.zero_grad()\n",
    "        loss, out = model(input_ids, mask, labels_mask, labels)\n",
    "        # print(loss)\n",
    "        # print('----')    \n",
    "        # loss = model(input_ids, mask, labels)\n",
    "        o.append(loss.cpu().detach().numpy())        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    return np.mean(o)\n",
    "\n",
    "\n",
    "def valid(model, data_loader,config):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    asw = []\n",
    "    loss_2 = []\n",
    "    outs = []\n",
    "    for _, txt in enumerate(data_loader):\n",
    "        input_ids=txt['input_ids'].to(config['device'])\n",
    "        mask=txt['attention_mask'].to(config['device'])\n",
    "        labels = txt['labels'].to(config['device']) \n",
    "        labels_mask = txt['label_attention_mask'].to(config['device'])\n",
    "        answer=txt['summary']\n",
    "\n",
    "        loss, out = model(input_ids, mask, labels_mask, labels)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        loss2 = loss_func(out, answer, config)\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        loss_2.append(loss2)\n",
    "        asw.append(answer)\n",
    "        outs.append(out)\n",
    "    # print(losses)\n",
    "    # print(loss_2)\n",
    "    return losses, np.vstack(asw), loss_2, np.vstack(outs)\n",
    "\n",
    "\n",
    "def run(config):\n",
    "    MODEL = config['model']\n",
    "    df = pd.read_csv('/content/drive/MyDrive/Coleridge_Initiative/input/v6_data_summary.csv')\n",
    "    df_small = df.drop_duplicates(subset=['id']).sample(frac = FRAC, random_state = 13).reset_index(drop=True)\n",
    "    train_df, val_df = train_test_split(df_small, random_state = 13, test_size = 0.1)\n",
    "    tr = train_df.reset_index(drop=True)\n",
    "    vl = val_df.reset_index(drop=True)\n",
    "\n",
    "    tokenizer = token.from_pretrained(MODEL)\n",
    "\n",
    "    tr_dataset = CI_Dataset(tr,tokenizer,config)\n",
    "    vl_dataset = CI_Dataset(vl,tokenizer,config)\n",
    "\n",
    "    tr_loader = DataLoader(tr_dataset, batch_size = config['batch_size'], shuffle = True, num_workers = 4)\n",
    "    vl_loader = DataLoader(vl_dataset, batch_size = 1, num_workers = 4)\n",
    "\n",
    "    model = CI(config).to(config['device'])\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = config['learning_rate'])\n",
    "    bar =  tqdm(range(config['epoch']))\n",
    "    wandb.watch(model) \n",
    "    best_jaccard = 0\n",
    "    for e in bar:          \n",
    "        tl = train(model, tr_loader, optimizer, config)      \n",
    "        l, _, j, _ = valid(model, vl_loader, config)    \n",
    "        jaccard = np.mean(j)\n",
    "        bar.set_description(f'Valid loss: {np.mean(l)}, Jaccard Loss: {jaccard}, Epoch: {e + 1}')\n",
    "        wandb.log(\n",
    "                  {'epoch': e,\n",
    "                   \"tr_loss\": np.mean(tl), \n",
    "                   'vl loss': np.mean(l),\n",
    "                   'jaccad': jaccard\n",
    "                \n",
    "                   }\n",
    "                  )\n",
    "        \n",
    "        if jaccard > best_jaccard:\n",
    "            print('\\n')\n",
    "            print(f'Best jaccard save: {jaccard}, prev: {best_jaccard}')\n",
    "            type_model = config['type_model']\n",
    "            torch.save(model.state_dict(), f'/content/drive/MyDrive/Coleridge_Initiative/model/model_{MODEL}_{type_model}_{EPOCHS}_{BATCH}_{FRAC}_{make_current_date()}.pth')\n",
    "            best_jaccard = jaccard\n",
    "            # pass\n",
    "\n",
    "    # https://pytorch.org/tutorials/beginner/saving_loading_models.html    \n",
    "    # torch.save(model.state_dict(), f'/content/drive/MyDrive/Coleridge_Initiative/model/model_{MODEL}_{EPOCHS}_{BATCH}_{FRAC}_{make_current_date()}.pth')\n",
    "    wandb.finish()\n",
    "    torch.cuda.empty_cache()    \n",
    "    # return l,t, j, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1623781996680,
     "user": {
      "displayName": "beifa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtHChsX9hHYvtnlB_68A7jsW4UBJFWRmr3FDc9=s64",
      "userId": "10643883607469925396"
     },
     "user_tz": -180
    },
    "id": "6tzZX53_IIy1",
    "outputId": "beb6d50f-d79f-47ff-8219-afe53c69dc02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1396, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['id']).sample(frac = 0.1, random_state = 13).reset_index(drop=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ImU0XzvpHVU",
    "outputId": "352973f6-e31d-4f60-a7bc-bc8574df61fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "Valid loss: 0.4172400236129761, Jaccard Loss: 0.8069206096652397, Epoch: 1:   0%|          | 0/6 [14:23<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best jaccard save: 0.8069206096652397, prev: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.39969244599342346, Jaccard Loss: 0.8078605121564549, Epoch: 2:  17%|█▋        | 1/6 [29:29<1:12:01, 864.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best jaccard save: 0.8078605121564549, prev: 0.8069206096652397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.38450759649276733, Jaccard Loss: 0.8136352730624806, Epoch: 3:  33%|███▎      | 2/6 [44:29<58:28, 877.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best jaccard save: 0.8136352730624806, prev: 0.8078605121564549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.40780121088027954, Jaccard Loss: 0.8277392875841563, Epoch: 4:  50%|█████     | 3/6 [59:14<44:10, 883.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best jaccard save: 0.8277392875841563, prev: 0.8136352730624806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.43061497807502747, Jaccard Loss: 0.8317157252718113, Epoch: 5:  67%|██████▋   | 4/6 [1:14:20<29:28, 884.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Best jaccard save: 0.8317157252718113, prev: 0.8277392875841563\n"
     ]
    }
   ],
   "source": [
    "# l, t, j, o =\n",
    "# %%time\n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fT01FvDQQsLY"
   },
   "outputs": [],
   "source": [
    "# config={\n",
    "   \n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     'tex_max_len': 396,\n",
    "#     'asw_max_len': 12,\n",
    "#     'batch_size' : BATCH,\n",
    "# }\n",
    "\n",
    "# for txl in [332, 364, 396, 428, 460, 492, 524]:\n",
    "# for lr in [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]:\n",
    "# for lr in [0.01, 0.05, 0.08, 0.001, 0.005, 0.008, 0.0001]:\n",
    "# for lr in [0.0008, 0.00001, 0.00003]:\n",
    "# for txl in [396, 492, 524]:\n",
    "\n",
    "# config={\n",
    "#     'type_model': 'summary',\n",
    "#     \"learning_rate\": 0.001,\n",
    "#     \"architecture\": \"T5\",\n",
    "#     'model': 't5-small', #'t5-base', # t5-small\n",
    "#     \"dataset\": \"Coleridge Initiative \",\n",
    "#     'tex_max_len': 512,\n",
    "#     'summar_max_len': 18,\n",
    "#     'batch_size' : BATCH,\n",
    "#     'epoch':EPOCHS,\n",
    "#     'device': 'cuda'\n",
    "# }\n",
    "\n",
    "# for sml in [8, 12, 14, 18, 20, 24, 32]:\n",
    "#     config['learning_rate'] = 0.0003\n",
    "#     config['summar_max_len'] = sml\n",
    "#     wandb.init(project=\"ci\",\n",
    "#            config=config,\n",
    "#            name =  f\"{config['architecture']}_{config['model']}_{config['type_model']}\"+\\\n",
    "#                    f\"_LR:{config['learning_rate']}_BATCH:{BATCH}_FRAC_DATA:{FRAC}\" +\\\n",
    "#                    f\"_summ_len{config['summar_max_len']}\"\n",
    "#            )\n",
    "#     config_w = wandb.config    \n",
    "#     run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyG-m-oXuEa9"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/transformers/main_classes/model.html?highlight=generate#transformers.TFPreTrainedModel.generate\n",
    "# https://huggingface.co/blog/how-to-generate"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "t5_summary_torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
