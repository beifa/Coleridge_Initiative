{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "infernce_t5_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 229.47705,
      "end_time": "2021-06-11T08:38:39.973279",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-06-11T08:34:50.496229",
      "version": "2.2.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8CCmQ-bLhgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c8daebb-d962-495e-c492-d23093e1fb77"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY_c2kWfE1S6"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "304fvKEpCdx7",
        "outputId": "90b898e3-fb04-4dd0-ac3b-8d7979a55f69"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os, glob, re, gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import permutations \n",
        "from collections import defaultdict\n",
        "from transformers import (T5ForConditionalGeneration,\n",
        "                          AdamW,\n",
        "                          T5TokenizerFast as token)\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "pl.seed_everything(13)\n",
        "print(torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 13\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqp5LR9u9M6v"
      },
      "source": [
        "BATCH = 3\n",
        "LOC = False\n",
        "\n",
        "config={\n",
        "    \"architecture\": \"T5\",\n",
        "    'model': 't5-small',\n",
        "    # 'model' : 't5-base', 2 long time\n",
        "    \"dataset\": \"Coleridge Initiative \",\n",
        "    'tex_max_len': 396,\n",
        "    'asw_max_len': 12,\n",
        "    'batch_size' : BATCH,\n",
        "    'device': 'cuda'\n",
        "    }\n",
        "\n",
        "PATH_ORI_TRAIN = '/content/drive/MyDrive/Coleridge_Initiative/input/train.csv'\n",
        "PATH_ORI_JSON = '/content/drive/MyDrive/Coleridge_Initiative/input/train'\n",
        "PATH_ORI_JSON_TEST = '/content/drive/MyDrive/Coleridge_Initiative/input/test'\n",
        "PATH_SUB = '/content/drive/MyDrive/Coleridge_Initiative/input/sample_submission.csv'\n",
        "MODEL_PATH = '/content/drive/MyDrive/Coleridge_Initiative/model/model_check_predict.pth'\n",
        "TOKEN_PATH = '../input/ci-model-small/token'\n",
        "\n",
        "#kaggle\n",
        "# MODEL_PATH = '../input/ci-model-small/model_check_predict_small.pth'\n",
        "\n",
        "\n",
        "train = pd.read_csv(PATH_ORI_TRAIN)\n",
        "sub = pd.read_csv(PATH_SUB)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1_fSQqWCdyD"
      },
      "source": [
        "def clean_text(txt):\n",
        "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n",
        "\n",
        "def clean_text_dig(txt):\n",
        "    return re.sub('[^A-Za-z]+', ' ', str(txt).lower()).strip()\n",
        "\n",
        "def find_start(title, text):\n",
        "    start = re.search(r'\\b%s\\b' % title, text)\n",
        "    if start == None:\n",
        "        start = 0\n",
        "    else:\n",
        "        start = start.start()\n",
        "    return start\n",
        "\n",
        "# def jaccard_similarity(s1, s2):\n",
        "#     l1 = s1.split(\" \")\n",
        "#     l2 = s2.split(\" \")    \n",
        "#     intersection = len(list(set(l1).intersection(l2)))\n",
        "#     union = (len(l1) + len(l2)) - intersection\n",
        "#     return float(intersection) / union\n",
        "\n",
        "# from kaggle competition, owner \n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "\n",
        "def totally_clean_text(txt):\n",
        "    txt = clean_text(txt)\n",
        "    txt = re.sub(' +', ' ', txt)\n",
        "    return txt\n",
        "\n",
        "def make_interval(start:int, txt:str, interval: int)->str:\n",
        "    \"\"\"\n",
        "    start: int return re.search() count latter\n",
        "    txt: str text\n",
        "    interval: int len text \n",
        "\n",
        "    return: str text\n",
        "    \"\"\"\n",
        "    words = txt.split()\n",
        "    len_interval = interval * 2\n",
        "    start = len(txt[:start].split())    \n",
        "    if (start - interval) > 0:\n",
        "        start = start - interval\n",
        "        if start + len_interval < len(words):\n",
        "            words = words[start: start + len_interval]\n",
        "        else:              \n",
        "            words = words[start: len(words)]\n",
        "    else:      \n",
        "        words = words[0: len_interval]\n",
        "    return ' '.join(words)\n",
        "\n",
        "def count_answer(df: pd.DataFrame)-> dict:\n",
        "    \"\"\"\n",
        "    problem 1:\n",
        "         its memory and we skip title len < 4\n",
        "         all 14316\n",
        "         get 5391\n",
        "\n",
        "\n",
        "    return dict    \n",
        "    \"\"\"\n",
        "    df['len_title'] = df.section_title.apply(lambda x: len(x.split()))\n",
        "    #need more clear title skip error\n",
        "    qwest = df[df.len_title >= 4].section_title.values\n",
        "    txt = df.text.str.cat()\n",
        "    txt = clean_text(txt)\n",
        "    tmp = {}\n",
        "    for qw in qwest:\n",
        "        qw = clean_text_dig(qw)  \n",
        "        if qw in txt:\n",
        "            start = find_start(qw, txt)\n",
        "            tmp['section_title'] = qw  \n",
        "            tmp['text'] = make_interval(start, txt, config['tex_max_len'])\n",
        "    return tmp"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snZvpSOKNOMo",
        "outputId": "1d18e532-b206-4477-f06d-9e3959ee0518"
      },
      "source": [
        "# papers = {}\n",
        "# # loads only sub json, all data load not need for inference\n",
        "# for json_id in tqdm(sub['Id'].unique()):\n",
        "#     with Path(PATH_ORI_JSON, json_id + '.json').open('r') as jsn:\n",
        "#         cur_jsn = json.load(jsn)\n",
        "#         tmp_df = pd.DataFrame(cur_jsn)\n",
        "#         dct = count_answer(tmp_df)\n",
        "#         # if len(dct) > 0:\n",
        "#         #     papers[json_id] = [dct]            \n",
        "# len(papers)\n",
        "\n",
        "# we add for hide folder kaggle data\n",
        "papers = {}\n",
        "for json_id in sub['Id']:\n",
        "    with Path(PATH_ORI_JSON_TEST, json_id + '.json').open('r') as jsn:\n",
        "        paper = json.load(jsn)\n",
        "        tmp_df = pd.DataFrame(paper)\n",
        "        dct = count_answer(tmp_df)\n",
        "        papers[json_id] = [dct]\n",
        "len(papers)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoiWmUAbCdyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56929038-02d5-46ee-e834-df6c1e09642b"
      },
      "source": [
        "all_labels = set()\n",
        "\n",
        "for label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n",
        "    all_labels.add(str(label_1).lower())\n",
        "    all_labels.add(str(label_2).lower())\n",
        "    all_labels.add(str(label_3).lower())\n",
        "\n",
        "print(f'No. different labels: {len(all_labels)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. different labels: 180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQVOQHxaCdyJ"
      },
      "source": [
        "# +- 1h time\n",
        "papers2 = {}\n",
        "for json_id in tqdm(train['Id'].unique()):\n",
        "    with Path(PATH_ORI_JSON, json_id + '.json').open('r') as jsn:\n",
        "        paper = json.load(jsn)\n",
        "        papers2[json_id] = paper\n",
        "\n",
        "\n",
        "for json_id in sub['Id']:\n",
        "    with Path(PATH_ORI_JSON_TEST, json_id + '.json').open('r') as jsn:\n",
        "        paper = json.load(jsn)\n",
        "        papers2[json_id] = paper\n",
        "        \n",
        "len(papers2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13PeVhy1CdyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c77dff64-2699-4410-d379-1e56c8a96c3a"
      },
      "source": [
        "literal_preds = []\n",
        "\n",
        "for paper_id in sub['Id']:\n",
        "    paper = papers2[paper_id]\n",
        "    text_1 = '. '.join(section['text'] for section in paper).lower()\n",
        "    text_2 = totally_clean_text(text_1)\n",
        "    \"\"\"\n",
        "    two way clear txt    \n",
        "    \"\"\"\n",
        "    \n",
        "    labels = set()\n",
        "    for label in all_labels:\n",
        "        if label in text_1 or label in text_2:\n",
        "            labels.add(clean_text(label))\n",
        "    \n",
        "    literal_preds.append('|'.join(labels))\n",
        "\n",
        "literal_preds"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adni|alzheimer s disease neuroimaging initiative adni',\n",
              " 'common core of data|trends in international mathematics and science study|nces common core of data',\n",
              " 'sea lake and overland surges from hurricanes|slosh model|noaa storm surge inundation',\n",
              " 'rural urban continuum codes']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QOWMXp_CdyL"
      },
      "source": [
        "class CI(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if LOC:\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained('../input/ci-model-small/model', return_dict = True)\n",
        "        else:\n",
        "            self.model = T5ForConditionalGeneration.from_pretrained(config['model'], return_dict = True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels):\n",
        "        out = self.model(input_ids = input_ids,\n",
        "                    attention_mask = attention_mask,\n",
        "                    labels = labels\n",
        "                    )\n",
        "        return out.loss, out.logits\n",
        "\n",
        "\n",
        "def make_pred(question:str, pre_model, tokenizer)->str:\n",
        "    encode_test = tokenizer(question['question'],\n",
        "                        question['text'],    \n",
        "                        max_length = config['tex_max_len'], \n",
        "                        padding= 'max_length',\n",
        "                        truncation = False,#'only_second',\n",
        "                        return_attention_mask = True,\n",
        "                        add_special_tokens =True,\n",
        "                        return_tensors = 'pt'\n",
        "                        )\n",
        "    gen_ids = pre_model.model.generate(\n",
        "        input_ids = encode_test['input_ids'],\n",
        "        attention_mask = encode_test['attention_mask'],\n",
        "        num_beams = 5,\n",
        "        no_repeat_ngram_size = 1,            \n",
        "        num_return_sequences = 1,     \n",
        "        do_sample=True,      \n",
        "        top_k=0,\n",
        "        top_p=0.92,       \n",
        "        max_length = config['asw_max_len'],\n",
        "        repetition_penalty = 2.5,\n",
        "        length_penalty =0.5,               \n",
        "        early_stopping = True,\n",
        "        use_cache = True\n",
        "        )\n",
        "\n",
        "\n",
        "    decode = [\n",
        "              tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "              for ids in gen_ids\n",
        "              ]\n",
        "    return ' '.join(decode)\n",
        "    \n",
        "\n",
        "def make_interval(start:int, txt:str, interval: int)->str:\n",
        "    \"\"\"\n",
        "    start: int return re.search() count latter\n",
        "    txt: str text\n",
        "    interval: int len text \n",
        "\n",
        "    return: str text\n",
        "    \"\"\"\n",
        "    words = txt.split()\n",
        "    len_interval = interval * 2\n",
        "    start = len(txt[:start].split())    \n",
        "    if (start - interval) > 0:\n",
        "        start = start - interval\n",
        "        if start + len_interval < len(words):\n",
        "            words = words[start: start + len_interval]\n",
        "        else:              \n",
        "            words = words[start: len(words)]\n",
        "    else:      \n",
        "        words = words[0: len_interval]\n",
        "    return ' '.join(words)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-GTO5s8CdyM"
      },
      "source": [
        "\"\"\"\n",
        "{'d0fa7568-7d8e-4db9-870f-f9c6f668c17b': [\n",
        "        {'section_title': 'What is this study about?',\n",
        "        'text': 'This study used data from the National Education Longitudinal Study (NELS:88)...\n",
        "        }],\n",
        "},\n",
        "\"\"\"\n",
        "def makedata(title, text, model, tokenizer, skip_title = True):\n",
        "    data = {'question':title,\n",
        "            'text': text,\n",
        "           }  \n",
        "    y_ = make_pred(data, model, tokenizer)\n",
        "    y_ = clean_text(y_)\n",
        "    return y_\n",
        "model = CI(config)\n",
        "MODEL = config['model']  \n",
        "model.load_state_dict(torch.load(MODEL_PATH))\n",
        "if LOC:    \n",
        "    tokenizer = token.from_pretrained(TOKEN_PATH)\n",
        "else:\n",
        "    tokenizer = token.from_pretrained(config['model'])\n",
        "model.eval()\n",
        "labels = []\n",
        "for paper_id in sub['Id']:\n",
        "    paper = papers[paper_id]  \n",
        "    label = []\n",
        "    for dct in paper:\n",
        "        if len(dct) > 0:\n",
        "            title = dct['section_title']\n",
        "            txt = dct['text']\n",
        "            if (title  != '') and (txt  != ''):\n",
        "                y_ = makedata(title, txt, model, tokenizer)        \n",
        "            else:\n",
        "                y_ = ''\n",
        "        else: y_ = ''\n",
        "        if y_ != '': label.append(y_)\n",
        "        gc.collect()\n",
        "    labels.append(np.unique(label))  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otq2hwz4CdyP"
      },
      "source": [
        "filtered_bert_labels = []\n",
        "\n",
        "for lab in labels:\n",
        "    filtered = []    \n",
        "    for label in sorted(lab, key=len):  \n",
        "        label = clean_text(label)\n",
        "        if len(filtered) == 0 or all(jaccard(label, got_label) < 0.75 for got_label in filtered):\n",
        "            filtered.append(label)    \n",
        "    filtered_bert_labels.append('|'.join(filtered))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrHAKXi2CdyQ"
      },
      "source": [
        "final_predictions = []\n",
        "for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n",
        "    if literal_match:\n",
        "        final_predictions.append(literal_match)\n",
        "    else:\n",
        "        final_predictions.append(bert_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvX2Ga96CdyQ"
      },
      "source": [
        "\n",
        "sample_submission = pd.read_csv(PATH_SUB)\n",
        "sample_submission['PredictionString'] = final_predictions\n",
        "sample_submission.to_csv(f'submission.csv', index=False)\n",
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}